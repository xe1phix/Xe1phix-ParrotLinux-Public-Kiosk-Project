SPEED COMPUTING
CURRENT COURSE

The aim of the tutorial is to give an overview on optimization and fast computation techniques for C/C++ programmers.
For this purpose, we will consider the structure of modern microprocessors and the features they bring to the computing process.

OPTIMIZATION OF CLASSICAL CALCULATIONS

Agner Fog is the best at this: https://agner.org
For loop deployment, processor cache lines, Intel microarchitecture, and SIMD computation, refer to his book series:
https://www.agner.org/optimize/


SUPERSCALAR MICROPROCESSORS

In superscalar microprocessors there is a high degree of redundancy of computational nodes.
There are several integer and real ALUs, there is a branch prediction block, there is a shadow register file, and a bunch of other things are duplicated.
This makes it possible to break the sequential code into chunks and execute them in parallel.

To be done

MEMORY BARRIERS

A memory barrier is a way of controlling the unscheduled execution of instructions by the processor and/or compiler.
It is counter-intuitive to execute code out of order. The code must be executed in the order in which it is written (in fact, it is not).
Operations on data not explicitly related (!) to each other may occur either out of order or all at once
within the same kernel, using the massive redundancy of the processor's computing nodes.
The memory barrier, on the other hand (to a rough approximation), makes the code execute the way it is written on the screen (sequentially and predictably),
rather than the way the processor and compiler want it to (in a counter-intuitive but more optimal order).
That is, it ensures that the code before the memory barrier is partially or completely executed, by the time the barrier instruction completes.
The barrier is usually an assembly instruction (i.e., present in the processor's instruction system).
The barrier is usually a "there is an implicit link between this data" hint, but not necessarily.

There are three memory models for atomics in C++:
1. relaxed: it is only guaranteed that operations will be executed atomically. In what order is the question.
- a variable modification will "appear" in another thread not immediately
- thread2 will "see" the values of the same variable in the same order as it was modified in thread1
- the order of modifications of the variables in thread1 won't remain in thread2
relaxed variables can be used as counters or stop flags.
The fastest and most unreliable memory model.
The DBMS transaction model analogue is READ UNCOMMITTED.
2. sequential consistency, seq_cst: memory state is synchronized between all program threads.
- the order of modifications of different atomic variables in thread1 will be preserved in thread2
- all threads will see the same order of modification of all atomic variables.
  The modifications themselves can occur in different threads
- all memory modifications (not only modifications over atomic variables) in the thread1, executing store on the atomic variable
  will be visible after executing load of the same variable in thread2
The slowest and most reliable memory model.
The counterpart of the DBMS transaction model is SERIALIZED.
3. acquire/release: pair synchronization.
- modification of an atomic variable with release will be instantly visible in another thread performing the same atomic variable reading with acquire
- all memory modifications in thread1 performing atomic variable write with release,
  will be visible after reading the same variable with acquire in thread2
- the processor and compiler cannot move memory write operations below the release operation in thread1,
  and cannot move above a read operation from memory above an acquire operation in thread2
Allows to do synchronization only between two threads (as opposed to all threads in thread1 and thread2).

https://habr.com/ru/post/517918/
https://gcc.gnu.org/wiki/Atomic/GCCMM/AtomicSync
https://habr.com/ru/company/JetBrains-education/blog/523298/
https://habr.com/ru/post/545996/
https://habr.com/ru/post/546222/
https://habr.com/ru/post/546880/
https://elixir.bootlin.com/linux/latest/source/Documentation/memory-barriers.txt

TIME MEASUREMENT WITH NANOSECOND ACCURACY

Accurate timing on x86/x64 architecture is a voluminous topic.
On scalar processors with fixed frequency you can count strokes, but not on x64: the result will be different.

The same code can execute differently because of the different state of the nodes in the system, which (roughly) is determined by the total load on the system.
Determinism is absent because of:
- unscheduled execution of instructions
- superscalar architecture (conveyor, shadow registers, cache state and other redundancy)
- Floating frequency of both the processor and separate parts of the system (bus)

This is exacerbated by differences in implementations between generations of processors, between processors from different manufacturers,
within the same generation of the same manufacturer.
And on top of that, we work in a general-purpose OS, with different processes and kernel code influencing each other,
which adds to the chaos.

In short, despite the huge speeds, for real-time tasks x64 so-so :)

We can talk about probabilities: total cycle time, average iteration time, estimates from below and above.

There is chrono::high_resolution_timer in C++. But on different compilers, different operating systems and processors its accuracy varies by three orders of magnitude.
I've seen the most accurate value on Linux / Intel(R) Core(TM) i5-4690K CPU @ 3.50GHz / gcc 8.4, with an accuracy of 30 nanoseconds.
On an Intel Core i5 480M @ 2.67GHz / Windows 7 / MSVC 2017 the accuracy is about 4000 nanoseconds.
On an Intel Atom x7-Z8750 @ 1.6GHz / Windows 10 / MSVC 2017 accuracy of 320 nanoseconds.

On x64 you can measure the time with Timestamp Counter (TSC) - available on all x64 generations MSR register.
On recent generations, it contains the number of clock cycles of the fixed frequency timer since the processor has been reset.
The meaning of this value is different for different generations and processor manufacturers.
You can check the fixed timer frequency guarantee by the presence of the constant_tsc flag in /proc/cpuinfo.
Timer reading is performed by RDTSC/RDTSCP instructions, which in turn is priced in clock cycles.
The timers themselves affect execution: they change the state of the pipeline, they are subject to unscheduled execution,
affect the cache (because we put the read data somewhere).

The method of measurement is as follows:
1. Calculate RDTSC instruction price in tacts at Current* load:
- load the conveyor with the cycle from this instruction, calculate the average number of clock cycles per instruction
2. Get (empirically**) the current timer frequency:
- Measure several times how many clock cycles occur in one second***
Determine the price in nanoseconds of one clock, based on the data from 1 and 2.
4. Sampling the measured section by RDTSC instructions, taking into account the price of RDTSC itself
5. Interpret the multiple measurements.
* If load changes, RDTSC price may float - due to floating processor frequency, conveyor state.
** Accurately and universally obtaining the timer frequency is quite difficult, both because of the different meaning of the value itself
on different processors, and because of differences in the necessary input data and formulas.
https://stackoverflow.com/questions/42189976/calculate-system-time-using-rdtsc
So we cut the corner here, go into empiricism, but get fairly plausible values.
*** And here we rely on the sleep function implementation in the OS/compiler, which has a lot of noise.

Interpretation of measurements:
- zero time value means unscheduled execution of two RDTSCs in a row:
The execution of the measured section was blocked, and the processor had to execute the second measurement instead.
That is, in fact, this code was not executed for zero time, but on the contrary longer than usual.
- Large bursts are context switching and waiting for another thread to work out its quantum of time.
In addition, it may be migrations of threads between cores - both the time associated with it,
or timer mismatch between cores (there is no guarantee that they are in sync).
- Smaller bursts are waiting for data transfer through the bus in case of cache misses
- other values more or less correspond to the truth.

As we can see, the methodology is imperfect.
Strong noise prevents us from getting accurate timings, but it is still possible to get at least qualitative characteristics
of the code execution, and calculate the probability distributions of the times.

x86 processors have a built-in Performance Monitoring Unit (PMU),
among which there is a frequency independent register (Describe Coreclock register)

REAL-TIME TASKS IN THE UNIVERSAL AXES

The best OSRP is single-tasking. Any context switching introduces a delay in execution; the cost of that delay can be immeasurable.
In MS DOS, the context is switched only on interrupts; in RT-11, even when asynchronous I/O finishes.
In today's OSRPs, timers and synchronization primitives have been added.
This is the maximum service that the OSRP can allow.
Attempts to introduce multitasking into the OSRP degrade its performance.

There is a known realtime patch for Linux, it really improves the predictability of the scheduler, reduces the number of context switches
and reduces delays in the RV process. This is suitable for many microsecond precision tasks.
A visual representation of context switching jitter during interrupt handling: https://habr.com/ru/post/562636/

The most radical way to easily and cheaply get OSRP from regular Linux is to allocate separate cores to PS tasks (thread affinity).
All interrupt handling and system processes are moved to other cores.
This eliminates the very possibility to switch contexts for PS processes (except some obligatory interrupts).
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/8/html/tuning_guide/chap-general_system_tuning


HIGH-SPEED NETWORKS

To reduce latency (minimize response time), hacks such as Kernel Bypass are used:
processing the network stream bypassing the kernel, in userland. In particular, this is used in SolarFlare NICs (their term is OnLoading).
The entire TCP/IP stack is built into a library running in userland mode.
Interrupts from hardware are of course present and handled in kernel; but network filter (iptables) and all unnecessary things are excluded from the kernel packet adventure.
This removes unnecessary kernel/user context switches, inter-core locks (which e.g. may be squeezed by something else for our process),
buffer copying, the possibility of re-planning an overextended handler in the kernel, etc.
This is usually combined with the affinement of RV tasks (see above) on a dedicated kernel.
The library makes maximum use of spinlocks as interlocks.
To the programmer it looks like normal sockets.

Bonding describe.

RDMA - remote direct memory access (another machine on the network) is used to increase bandwidth.
In this case, copying into the memory of the computer takes place directly from the hardware receive buffer of the network card,
bypassing the CPU processing, bypassing the operating system as such (in fact, without generating interrupts).
The low latency here is a trailer, but it is most effective for a thick constant stream of data (e.g., in clustered DBMS).
An example of iron is Infiniband, libibverbs library.
It is used in interlinked clusters, clustered DBMSs.

A common place in high-speed networks is to eliminate buffer copying (zero copy).
On a gigabyte stream, the buffer copy time *already* reduces throughput by half.
On a 100GB stream, do the math for yourself.
Therefore, all calculations and data manipulation should be done directly in the transmit/receive buffers.

SHADOW BUFFERING/DOUBLE BUFFERING

Shadow buffering is used to achieve high throughput data transfer (to the network, when resetting to disk):
- several buffers are allocated, only one of them is active (ready for transfer) at one point in time
- inactive buffers are filled with data in separate streams
- data is transferred from the active buffer in the meantime
- when transmission from the active buffer is finished, it is marked as inactive; the next buffer ready for work is selected.

This is a particular case of the ring buffer idea.

READ MORE

Floating point calculations without error
https://habr.com/ru/post/523654/

Optimizing math calculations and the -ffast-math option in GCC 11
https://habr.com/ru/company/ruvds/blog/586386/

Fast parsing double
https://habr.com/ru/company/ruvds/blog/542640/
https://github.com/fastfloat/fast_float

Quick UTF8 validation
https://habr.com/ru/company/ruvds/blog/551060/
https://arxiv.org/pdf/2010.03090.pdf

ASM today
https://habr.com/ru/post/544786/

Ring buffer without modulo division
https://habr.com/ru/company/otus/blog/557310/

epoll and Windows IO Completion Ports: Practical Difference
https://habr.com/ru/company/infopulse/blog/415403/

What is the limit of the branch predictor? Tested on x86 and M1
https://habr.com/ru/company/selectel/blog/557410/

REFERENCES
1. Agner Fog, Optimization manuals https://www.agner.org/optimize/
2. Cost of operations in CPU clock cycles https://habr.com/ru/company/otus/blog/343566/
3. select / poll / epoll: the practical difference https://habr.com/ru/company/infopulse/blog/415259/
4. Evaluating the Cost of Atomic Operations onModern Architectures https://spcl.inf.ethz.ch/Publications/.pdf/atomic-bench.pdf
5. Intel Intrinsics Guide https://software.intel.com/sites/landingpage/IntrinsicsGuide/
6. Neon Intrinsics Reference https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/intrinsics
miscellaneous
blogs
https://easyperf.net/notes/
http://scrutator.me/

